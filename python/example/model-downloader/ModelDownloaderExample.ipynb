{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary imports, start spark and create our model downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:04:45) [MSC v.1900 32 bit (Intel)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "print(sys.version)\n",
    "\n",
    "from sparknlp.pretrained import ResourceDownloader\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"downloader-example\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# instantiate the downloader\n",
    "downloader = ResourceDownloader()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dummy spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some mock data to play with\n",
    "l = [\n",
    "  (1,'To be or not to be'),\n",
    "  (2,'This is it!')\n",
    "]\n",
    "\n",
    "data = spark.createDataFrame(l, ['docID','text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we intend to download a POS model by its name and language, which requires tokenized text. Hence, we create our tokenizer pipeline to get the data ready.\n",
    "Then, we add the POS along the other annotators and transform some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|docID|              text|            document|            sentence|               token|                 pos|\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    1|To be or not to be|[[document, 0, 17...|[[document, 0, 17...|[[token, 0, 1, To...|[[pos, 0, 1, TO, ...|\n",
      "|    2|       This is it!|[[document, 0, 10...|[[document, 0, 10...|[[token, 0, 3, Th...|[[pos, 0, 3, DT, ...|\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download directly - models\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\")\n",
    "\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "    \n",
    "# pos tagger\n",
    "pos = downloader.downloadModel(PerceptronModel, \"pos_fast\", \"en\")    \n",
    "    \n",
    "pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, pos])\n",
    "\n",
    "output = pipeline.fit(data).transform(data)\n",
    "output.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we download a Pipeline by its name and language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|docID|              text|            document|               token|              normal|               lemma|                 pos|\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    1|To be or not to be|[[document, 0, 17...|[[token, 0, 1, To...|[[token, 0, 1, to...|[[token, 0, 1, To...|[[pos, 0, 1, TO, ...|\n",
      "|    2|       This is it!|[[document, 0, 10...|[[token, 0, 3, Th...|[[token, 0, 3, th...|[[token, 0, 3, Th...|[[pos, 0, 3, DT, ...|\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download directly - pipeline models\n",
    "\n",
    "# simple pipeline with document assembler and tokenizer\n",
    "pipeline = downloader.downloadPipeline(\"pipeline_basic\", \"en\")\n",
    "pipeline.transform(data).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clear cache of recently downloaded pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test clearCache\n",
    "downloader.clearCache(\"pipeline_basic\", \"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use predefined BasicPipeline in order to annotate a dataframe with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|docID|              text|            document|               token|              normal|               lemma|                 pos|\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    1|To be or not to be|[[document, 0, 17...|[[token, 0, 1, To...|[[token, 0, 1, to...|[[token, 0, 1, To...|[[pos, 0, 1, TO, ...|\n",
      "|    2|       This is it!|[[document, 0, 10...|[[token, 0, 3, Th...|[[token, 0, 3, th...|[[token, 0, 3, Th...|[[pos, 0, 3, DT, ...|\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download predefined - pipelines\n",
    "from sparknlp.pretrained.pipeline.en import BasicPipeline\n",
    "\n",
    "basic_data = BasicPipeline.annotate(data, \"text\")\n",
    "basic_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also annotate a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': ['This world is made up of good and bad things'],\n",
       " 'lemma': ['This', 'world', 'be', 'make', 'up', 'of', 'good', 'and', 'bad', 'thing'],\n",
       " 'normal': ['this', 'world', 'is', 'made', 'up', 'of', 'good', 'and', 'bad', 'things'],\n",
       " 'pos': ['DT', 'NN', 'VBZ', 'VBN', 'RP', 'IN', 'JJ', 'CC', 'JJ', 'NNS'],\n",
       " 'token': ['This', 'world', 'is', 'made', 'up', 'of', 'good', 'and', 'bad', 'things']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# annotat quickly from string\n",
    "BasicPipeline().annotate(\"This world is made up of good and bad things\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we proceed to download a POS model, utilizing the PerceptronModel class to retrieve it.\n",
    "We do the same for the NER model.\n",
    "Then, we retrieve the Basic Pipeline and combine these models to use them appropriately meeting their requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|docID|              text|            document|               token|              normal|               lemma|                 pos|\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    1|To be or not to be|[[document, 0, 17...|[[token, 0, 1, To...|[[token, 0, 1, to...|[[token, 0, 1, To...|[[pos, 0, 1, TO, ...|\n",
      "|    2|       This is it!|[[document, 0, 10...|[[token, 0, 3, Th...|[[token, 0, 3, th...|[[token, 0, 3, Th...|[[pos, 0, 3, DT, ...|\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|docID|              text|            document|               token|              normal|               lemma|                 pos|                 ner|\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    1|To be or not to be|[[document, 0, 17...|[[token, 0, 1, To...|[[token, 0, 1, to...|[[token, 0, 1, To...|[[pos, 0, 1, TO, ...|[[named_entity, 0...|\n",
      "|    2|       This is it!|[[document, 0, 10...|[[token, 0, 3, Th...|[[token, 0, 3, th...|[[token, 0, 3, Th...|[[pos, 0, 3, DT, ...|[[named_entity, 0...|\n",
      "+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download predefined - models\n",
    "\n",
    "pos = PerceptronModel.retrieve()\n",
    "pos.setInputCols([\"document\", \"normal\"]).setOutputCol(\"pos\")\n",
    "\n",
    "ner = NerCrfModel.retrieve()\n",
    "ner.setInputCols([\"pos\", \"normal\", \"document\"]).setOutputCol(\"ner\")\n",
    "\n",
    "annotation_pipeline = BasicPipeline.retrieve()\n",
    "annotation_data = annotation_pipeline.transform(data)\n",
    "annotation_data.show()\n",
    "\n",
    "pos_tagged = pos.transform(annotation_data)\n",
    "ner_tagged = ner.transform(pos_tagged)\n",
    "ner_tagged.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
